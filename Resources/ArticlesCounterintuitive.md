# Counterintuitive articles

The following is a list of some papers showing counterintuitive results in machine learning, reverse sorted by year.
<!-- Free papers only-->

* [No Training Required: Exploring Random Encoders for Sentence Classification (2019)](https://arxiv.org/abs/1901.10444)

* [Elimination of All Bad Local Minima in Deep Learning (2019)](https://arxiv.org/abs/1901.00279)

* [Adding One Neuron Can Eliminate All Bad Local Minima (2018)](https://arxiv.org/abs/1805.08671)

* [Learning with Random Learning Rates (2018)](https://arxiv.org/abs/1810.01322)

* [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (2018)](https://arxiv.org/abs/1803.03635)

* [Intriguing Properties of Randomly Weighted Networks: Generalizing While Learning Next to Nothing (2018)](https://arxiv.org/abs/1802.00844)
  * The interpretation of the result is questionable.

* [A New Backpropagation Algorithm without Gradient Descent (2018)](https://arxiv.org/abs/1802.00027)

* [Generating Neural Networks with Neural Networks (2018)](https://arxiv.org/abs/1801.01952)

* [Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning (2018)](https://arxiv.org/abs/1712.06567)
  * This is also a stand-in for numerous related approaches, e.g. [evolutionary strategies](https://arxiv.org/abs/1703.03864), [PBT](https://arxiv.org/abs/1711.09846), [random search](https://arxiv.org/abs/1803.07055), [world models](https://arxiv.org/abs/1803.10122).

* [One pixel attack for fooling deep neural networks (2017)](https://arxiv.org/abs/1710.08864)

* [Training Feedforward Neural Networks with Standard Logistic Activations is Feasible (2017)](https://arxiv.org/abs/1710.01013)

* [Early Stopping without a Validation Set (2017)](https://arxiv.org/abs/1703.09580)

* [Understanding deep learning requires rethinking generalization (2016)](https://arxiv.org/abs/1611.03530)

* [Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures (2016)](https://arxiv.org/abs/1608.06037)

* [Random feedback weights support learning in deep neural networks (2014)](https://arxiv.org/abs/1411.0247)
  * http://www.breloff.com/no-backprop/

* [Intriguing properties of neural networks (2014)](https://arxiv.org/abs/1312.6199)

* [The dipping phenomenon (2012)](https://www.semanticscholar.org/paper/The-Dipping-Phenomenon-Loog-Duin/f9ce91b1b046af38b63f2e079a0442c6c6364cf6)
