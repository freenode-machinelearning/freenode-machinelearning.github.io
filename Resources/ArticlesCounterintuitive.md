# Counterintuitive articles

The following is a list of some papers showing counterintuitive results in machine learning, reverse sorted by year.
<!-- Free papers only-->

* [Generating Neural Networks with Neural Networks (2018)](https://arxiv.org/abs/1801.01952)

* [A New Backpropagation Algorithm without Gradient Descent (2018)](https://arxiv.org/abs/1802.00027)

* [Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning (2018)](https://arxiv.org/abs/1712.06567)
  * This is also a stand-in for numerous related approaches, e.g. [evolutionary strategies](https://arxiv.org/abs/1703.03864), [PBT](https://arxiv.org/abs/1711.09846), [random search](https://arxiv.org/abs/1803.07055), [world models](https://arxiv.org/abs/1803.10122).



* [Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures (2018)](https://arxiv.org/abs/1608.06037)

* [One pixel attack for fooling deep neural networks (2018)](https://arxiv.org/abs/1710.08864)

* [Early Stopping without a Validation Set (2017)](https://arxiv.org/abs/1703.09580)

* [Understanding deep learning requires rethinking generalization (2017)](https://arxiv.org/abs/1611.03530)

* [Random feedback weights support learning in deep neural networks (2014)](https://arxiv.org/abs/1411.0247)
  * http://www.breloff.com/no-backprop/

* [Intriguing properties of neural networks (2014)](https://arxiv.org/abs/1312.6199)
